<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>莫名的blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="莫名的blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="莫名的blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="莫名的blog">
  
    <link rel="alternate" href="/atom.xml" title="莫名的blog" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/css/home.css" >
  

  

  

  
  
  
    <link rel="stylesheet" href="/css/vdonate.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>



  <body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header">

	<!-- 背景图模式 -->
	

    
      <div id="intrologo" class="intro-logo">
    
 


    <canvas width="100%" height="100%"></canvas>
    <script>
      var c = document.getElementsByTagName('canvas')[0],
          x = c.getContext('2d'),
          w = window.innerWidth,
          h = window.innerHeight,
          pr = window.devicePixelRatio || 1,
          f = 90,
          q,
          m = Math,
          r = 0,
          u = m.PI*2,
          v = m.cos,
          z = m.random
      c.width = w*pr
      c.height = h*pr
      x.scale(pr, pr)
      x.globalAlpha = 0.6

      <!-- 折线Polyline背景 -->
      
        function i(){
            x.clearRect(0,0,w,h)
            q=[{x:0,y:h*.7+f},{x:0,y:h*.7-f}]
            while(q[1].x<w+f) d(q[0], q[1])
        }
        function d(i,j){   
            x.beginPath()
            x.moveTo(i.x, i.y)
            x.lineTo(j.x, j.y)
            var k = j.x + (z()*2-0.25)*f,
                n = y(j.y)
            x.lineTo(k, n)
            x.closePath()
            r-=u/-50
            x.fillStyle = '#'+(v(r)*127+128<<16 | v(r+u/3)*127+128<<8 | v(r+u/3*2)*127+128).toString(16)
            x.fill()
            q[0] = q[1]
            q[1] = {x:k,y:n}
        }
        function y(p){
            var t = p + (z()*2-1.1)*f
            return (t>h||t<0) ? y(p) : t
        }
        document.getElementById("intrologo").onclick = i
        document.getElementById("intrologo").ontouchstart = i
        i()

      <!-- 多边形trianglify背景 -->
      
    </script>
    

    
      <div id="homelogo" class="homelogo"> 
    

        
          <div class="homelogoback" >
            <h1><a href="#content" id="logo">莫名的blog</a></h1>
            <h3></h3>
            <h5>莫名</h5>
            <!-- <p><a href="https://github.com/iTimeTraveler" target="_blank">Github</a></p> -->
          </div>
        
    
    </div>
  </div>

  <!-- 自适应主页背景大图 -->
  

 <!-- home_logo_image居中 -->
 
    <script>
        var homelogodiv = document.getElementById("homelogo");
        if (document.all.homelogo.offsetWidth > document.body.clientWidth) {
          homelogodiv.style.width = document.body.clientWidth + "px";
          homelogodiv.style.marginLeft = document.body.clientWidth * -0.5 + "px";
        } else {
          homelogodiv.style.width = homelogodiv.clientWidth  + "px";
          homelogodiv.style.marginLeft = (homelogodiv.clientWidth)  * -0.5 + "px";
        }
    </script>
  

  <div class="intro-navigate">
      <p class="navigater-list">
        
          <a id="beautifont" class="main-nav-link" href="/">Home</a>
        
          <a id="beautifont" class="main-nav-link" href="/archives">Archives</a>
        
          <a id="beautifont" class="main-nav-link" href="/categories">Categories</a>
        
          <a id="beautifont" class="main-nav-link" href="/tags">Tags</a>
        
          <a id="beautifont" class="main-nav-link" href="/about">About</a>
        
      </p>
  </div>

</header>
  
  <div id="container">
    <div id="wrap">
      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;">
  
    <article id="post-Sarsa"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/06/Sarsa/">Sarsa</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/06/Sarsa/" class="article-date">
	  <time datetime="2017-12-06T14:42:44.000Z" itemprop="datePublished">2017-12-06</time>
	</a>

      
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


<h1 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h1><p>Sarsa与Q-Learning类似，都是通过构建一个Q表来记录agent每个状态下采取相应动作的回报，其不同处如下：<br>
1、Q-Learning $$\max_{{a}'}Q({s}', {a}')$$变成Sarsa的$$Q({s}', {a}')$$
2、Q-Learning在每一步确定下一步的状态，Q-Learning中agent下一步可能选择的不是${a}'$，而Sarsa同时确定了下一步的状态和行为。
</p>
<p>总的来说，Q-Learning表现的更加激进，总是贪心地选择最大回报的那个行为，但并不一定说到做到，而Sarsa则更为保守，学着自己在做的事情。</p>
<p><img src="https://morvanzhou.github.io/static/results/ML-intro/s4.png" alt=""></p>
<p><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/2_Q_Learning_maze" target="_blank" rel="external">2_Q_Learning_maze</a></p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-Q-Learning"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/03/Q-Learning/">Q Learning</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/03/Q-Learning/" class="article-date">
	  <time datetime="2017-12-03T07:21:09.000Z" itemprop="datePublished">2017-12-03</time>
	</a>

      
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="Q-Learningg"><a href="#Q-Learningg" class="headerlink" title="Q Learningg"></a>Q Learningg</h1><p>该算法会构建一个Q表，行为状态s,列为行为a,其中每一项的值为对应状态下选择相应行为的回报，该回报通过动态规划计算而来。</p>
<p>每一回合，给定一个初始状态，从该状态开始，选择行为，转移到另一个状态，再选择行为，依次类推，直到遇到结束状态本回合结束。</p>
<p>每次选择行为时，该算法会根据给定的一个概率$\varepsilon$,随机选择行为或选择未来回报最大的行为。</p>
<p>选择行为以后，会根据env返回的回报和状态，更新Q表和agent的当前状态。</p>
<p>其中更新Q表的公式如下：</p>
$$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \displaystyle  \max_{{a}'}Q({s}', {a}') - Q(s, a)] $$
<p>其中：</p>
<ul>
<li>$Q(s, a)$为状态$s$行为$a$的回报值；</li>
<li>$\alpha$为learning rate；</li>
<li>$r + \gamma \displaystyle  \max_{{a}'}Q({s}', {a}')$为实际回报；</li>
<li>$Q(s, a)$为预测回报；</li>
</ul>
<p><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/2-1-1.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-RLClassfication"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/02/RLClassfication/">RLClassfication</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/02/RLClassfication/" class="article-date">
	  <time datetime="2017-12-02T07:07:41.000Z" itemprop="datePublished">2017-12-02</time>
	</a>

      
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Reingorcement-Learning"><a href="#Reingorcement-Learning" class="headerlink" title="Reingorcement Learning"></a>Reingorcement Learning</h1><h2 id="分类1"><a href="#分类1" class="headerlink" title="分类1"></a>分类1</h2><h3 id="model-feel-rl-不理解环境"><a href="#model-feel-rl-不理解环境" class="headerlink" title="model-feel rl 不理解环境"></a>model-feel rl 不理解环境</h3><ul>
<li>Q Learning</li>
<li>Sarsa</li>
<li>Policy Cradients</li>
</ul>
<h3 id="model-based-rl-理解环境"><a href="#model-based-rl-理解环境" class="headerlink" title="model-based rl 理解环境"></a>model-based rl 理解环境</h3><p>比model-free多了一个虚拟环境，model-free中的方法也适用</p>
<h2 id="分类2"><a href="#分类2" class="headerlink" title="分类2"></a>分类2</h2><h3 id="Policy-based-RL-基于概率"><a href="#Policy-based-RL-基于概率" class="headerlink" title="Policy-based RL 基于概率"></a>Policy-based RL 基于概率</h3><ul>
<li>policy gradients et ta.</li>
</ul>
<h3 id="Value-Based-RL-基于价值"><a href="#Value-Based-RL-基于价值" class="headerlink" title="Value-Based RL 基于价值"></a>Value-Based RL 基于价值</h3><p>对连续的动作无能为力</p>
<ul>
<li>Q learning</li>
<li>Sarsa</li>
</ul>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>Actor基于概率生成动作，Critic根据给定的动作计算价值</p>
<h2 id="分类3"><a href="#分类3" class="headerlink" title="分类3"></a>分类3</h2><h3 id="Monte-Carlo-update-回合更新"><a href="#Monte-Carlo-update-回合更新" class="headerlink" title="Monte-Carlo update 回合更新"></a>Monte-Carlo update 回合更新</h3><p>游戏开始-&gt;游戏结束-&gt;更新</p>
<ul>
<li>基础版 policy gradients</li>
<li>Monte-Carlo Learning</li>
</ul>
<h3 id="Temporal-Difference-update"><a href="#Temporal-Difference-update" class="headerlink" title="Temporal-Difference update"></a>Temporal-Difference update</h3><p>游戏开始-&gt;更新-&gt;更新-&gt;…-&gt;游戏结束<br>每一步都会更新，边玩边更新，更有效率，大部分增强学习算法都是单步更新方法</p>
<ul>
<li>Q Learning</li>
<li>Sarsa</li>
<li>升级版Policy Gradients</li>
</ul>
<h2 id="分类4"><a href="#分类4" class="headerlink" title="分类4"></a>分类4</h2><h3 id="On-Policy-在线学习"><a href="#On-Policy-在线学习" class="headerlink" title="On-Policy 在线学习"></a>On-Policy 在线学习</h3><p>自己玩，边玩边学习</p>
<ul>
<li>Sarsa</li>
<li>Sarsa($\lambda$)</li>
</ul>
<h3 id="Off-Policy-离线学习"><a href="#Off-Policy-离线学习" class="headerlink" title="Off-Policy 离线学习"></a>Off-Policy 离线学习</h3><p>既可以自己玩，也可以看着别人玩</p>
<ul>
<li>Q Learning</li>
<li>Deep Q Network</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RL/">RL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reingorcement-Learning/">Reingorcement Learning</a></li></ul>

    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-MemNN"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/11/06/MemNN/">Memory network (MemNN) &amp; End to End memory network (MemN2N) &amp; Dynamic memory network</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/11/06/MemNN/" class="article-date">
	  <time datetime="2017-11-06T15:29:04.000Z" itemprop="datePublished">2017-11-06</time>
	</a>

      
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="记忆网络（Memory-network）"><a href="#记忆网络（Memory-network）" class="headerlink" title="记忆网络（Memory network）"></a>记忆网络（Memory network）</h1><p>虚拟助理在回答单个问句时表现不赖，但是在多轮对话中表现差强人意，以下例子说明我们面临着什么挑战：</p>
<ul>
<li>Me: Can you find me some restaurants?</li>
<li>Assistance: I find a few places within 0.25 mile. The first one is Caffé Opera on Lincoln Street. The second one is …</li>
<li>Me: Can you make a reservation at the first restaurant?</li>
<li>Assistance: Ok. Let’s make a reservation for the Sushi Tom restaurant on the First Street.</li>
</ul>
<p>为什么虚拟助理不能按照我的指令预订Caffé Opera？那是因为虚拟助理并不能记住我们的对话，她只是简单地回答我们的问题而不考虑向前谈话的上下午。因此，她所能做的只是找到与词“First”相关的餐厅（一间位于First Street的餐厅）。记忆网络（Memory Networks）通过记住处理过的信息来解决该问题。</p>
<blockquote>
<p> The description on the memory networks (MemNN) is based on <a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="external">Memory networks, Jason Weston etc.</a></p>
</blockquote>
<p>考虑以下语句和问句“Where is the milk now?”：</p>
<ol>
<li>Joe went to the kitchen.</li>
<li>Fred went to the kitchen.</li>
<li>Joe picked up the milk.</li>
<li>Joe traveled to the office.</li>
<li>Joe left the milk.</li>
<li>Joe went to the bathroom.</li>
</ol>
<h2 id="存信息于记忆"><a href="#存信息于记忆" class="headerlink" title="存信息于记忆"></a>存信息于记忆</h2><p>首先，保存句子在记忆m中：</p>
<table>
<thead>
<tr>
<th>Memory slot \(m_i\)</th>
<th>Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Joe went to the kitchen.</td>
</tr>
<tr>
<td>2</td>
<td>Fred went to the kitchen. </td>
</tr>
<tr>
<td>3</td>
<td>Joe picked up the milk.  </td>
</tr>
<tr>
<td>4</td>
<td>Joe traveled to the office.  </td>
</tr>
<tr>
<td>5</td>
<td>Joe left the milk.</td>
</tr>
<tr>
<td>6</td>
<td>Joe went to the bathroom.</td>
</tr>
</tbody>
</table>
<h2 id="回答问句（Answering-a-query）"><a href="#回答问句（Answering-a-query）" class="headerlink" title="回答问句（Answering a query）"></a>回答问句（Answering a query）</h2><p>回答一个问句\(q\)，首先通过打分函数\(s_{0}\)计算出与\(q\)最相关的句子\(m_{01}\)。然后将句子\(m_{01}\)与\(q\)结合起来形成新的问句\([q, m_{01}]\)，并且定位最高分的下一个句子\(m_{01}\)。最后形成另外一个问句\([q, m_{01}, m_{02}]\)。此时我们并不是用该问句去查询下一个句，而是通过另一个打分函数定位一个词\(w\)。以上述例子来说明该过程。</p>
<p>回答问句\(q\)“where is the milk now?”，我们基于以下式子计算第一个推断：</p>
<p>$$ \mathop{\arg\min}_{i=1,…,N} $$</p>
<p>$$ o<em>{1} = \mathop{\arg\min}</em>{i=1,…,N}s<em>{0}(q, m</em>{i}) $$</p>
<p>其中，\(s_{0}\)是计算输入\(x\)与\(m_{i}\)匹配分数的函数，\(o_{1}\)是记忆\(m\)中最佳匹配索引。这里\(m_{01}\)是第一个推断中最好的匹配句：“Joe left the milk.”。<br>然后，基于\([q: “where is the milk now”, m_{01}: “Joe left the milk.”]\)$$o<em>2<br> = \mathop{\arg\max}</em>{i=1,…,N}s<em>0([q, m</em>{01}], m<em>i)$$其中\(m</em>{02}\)是“Joe traveled to the office.”。<br>结合问句和推导的结果记为\(o\)：$$o = [q, m<em>{01}, m</em>{02}] = [“where is the milk now”,” Joe left the milk.”,” Joe travelled to the office.”]$$生成最终的答复\(r\)：$$r = \mathop{\arg\max}_{w \in W}s<em>r([q, m</em>{01}, m_{02}], w)$$其中\(W\)是字典中的所有词，\(s<em>r\)是另外一个计算\([q, m\</em>{01}, m_{02}]\)和词\(w\)的匹配度。在我们的例子中，最后的回答\(r\)是“office”。</p>
<h2 id="编码输入（Encoding-the-input）"><a href="#编码输入（Encoding-the-input）" class="headerlink" title="编码输入（Encoding the input）"></a>编码输入（Encoding the input）</h2><p>我们利用词袋法（bags of words）表示输入文本。首先，我们以大小为\(\left|W\right|\)开始。<br>用词袋法对问句“where is the milk now”编码：</p>
<table>
<thead>
<tr>
<th>Vocaulary</th>
<th>…</th>
<th>is</th>
<th>Joe</th>
<th>left</th>
<th>milk</th>
<th>now</th>
<th>office</th>
<th>the</th>
<th>to</th>
<th>travelled</th>
<th>where</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr>
<td>where is the milk now</td>
<td>…</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>…</td>
</tr>
</tbody>
</table>
<p></p><p style="text-align:center">“where is the milk now”=(…,1,0,0,1,1,0,1,0,0,1,…)</p><br>为了达到更好的效果，我们分别用三个词集编码\(q\)，\(m<em>{01}\)和\(m</em>{02}\)，即\(q\)中的词“Joe”编码为“Joe<em>1”，\(m</em>{01}\)中同样的词编码为“Joe<em>2”：<br>\(q\): Where is the milk now?<br>\(m</em>{01}\): Joe left the milk.<br>\(m<em>{02}\): Joe travelled to the office.<br>编码上述\(q, m</em>{01}, m_{02}\)：<p></p>
<table>
<thead>
<tr>
<th>…</th>
<th>Joe_1</th>
<th>milk_1</th>
<th>…</th>
<th>Joe_2</th>
<th>milk_2</th>
<th>…</th>
<th>Joe_3</th>
<th>milk_3</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>0</td>
<td>1</td>
<td></td>
<td>1</td>
<td>1</td>
<td></td>
<td>1</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
<p>因此，每一句变换为大小为\(3\left|W\right|\)的编码。</p>
<h2 id="计算打分函数（Compute-the-scoring-function）"><a href="#计算打分函数（Compute-the-scoring-function）" class="headerlink" title="计算打分函数（Compute the scoring function）"></a>计算打分函数（Compute the scoring function）</h2><p>我们用词嵌入\(U\)转换\(3\left|W\right|\)词袋编码的句子为大小为\(n\)的词嵌入表示。计算打分函数\(s_0\)和\(s_r\)：$$s_0(x, y)n = \Phi_x(x)^TU_0^TU_0\Phi_y(y)$$ $$s_r(x, y)n = \Phi_x(x)^TU_r^TU_r\Phi_y(y)$$其中\(U_0\)和\(U_r\)由边缘损失函数训练得到，\(\phi(m_i)\)转换句子\(m_i\)为词袋表示。</p>
<h2 id="边缘损失函数（Margin-loss-function）"><a href="#边缘损失函数（Margin-loss-function）" class="headerlink" title="边缘损失函数（Margin loss function）"></a>边缘损失函数（Margin loss function）</h2><p>用边缘损失函数训练\(U_0\)和\(U<em>r\)中的参数：<br>$$\sum\limits</em>{\overline{f}\not=m_{01}}\max(0, \gamma - s<em>0(x, m</em>{01}) + s<em>0(x, \overline{f})) + $$ $$\sum\limits</em>{\overline{f}\not=m_{02}}\max(0, \gamma - s<em>0(\left[x, m</em>{01}\right], m_{02}) + s<em>0(\left[x, m</em>{01}\right], \overline{f^\prime})) + $$ $$\sum\limits_{\overline{r}\not=r}\max(0, \gamma - s<em>0(\left[x, m</em>{01}, m_{02}\right], r) + s<em>0(\left[x, m</em>{01}, m_{02}\right], \overline{r}))$$其中\(\overline{f}\)，\(\overline{f^\prime}\)和\(\overline{r}\)是真实标签外的其它可能预测值。即当错误回答的分数大于正确回答的分数减\(\gamma\)时增加边缘损失。</p>
<h2 id="大记忆网络（Huge-memory-networks）"><a href="#大记忆网络（Huge-memory-networks）" class="headerlink" title="大记忆网络（Huge memory networks）"></a>大记忆网络（Huge memory networks）</h2><p>对于记忆规模较大的的系统，计算每个记忆的分数较昂贵。其它可选方案为，计算完词嵌入\(U_0\)后，运用K-clustering将词嵌入空间分为K类。然后将每个输入\(x\)映射到相应类中，并在类空间中进行推测而不是在全部记忆空间中。</p>
<h1 id="端到端记忆网络（End-to-End-memory-network-MemN2N）"><a href="#端到端记忆网络（End-to-End-memory-network-MemN2N）" class="headerlink" title="端到端记忆网络（End to End memory network, MemN2N）"></a>端到端记忆网络（End to End memory network, MemN2N）</h1><blockquote>
<p>The description, as well as the diagrams, on the end to end memory networks (MemN2N) are based on <a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="external">End-To-End Memory Networks, Sainbayar Sukhbaatar etc.</a>.</p>
</blockquote>
<p>以一句问句“where is the milk now?”开始，并用大小为\(V\)的词袋表示（其中\(V\)为词典大小）。简单地，用词嵌入矩阵\(B(d\times V)\)转换上述向量为\(d\)维词嵌入。$$u = embedding_B(q)$$ 对于每个记忆项\(x_i\)，用另一个词嵌入矩阵\(A(d\times V)\)转换为d维向量\(m_i\)。$$m_i = embedding_A(x_i)$$<br><img src="https://yqfile.alicdn.com/0ec1fa6d28aff9d51ab6e78eda7123334f703012.png" alt="max10"><br>通过计算\(u\)与每个记忆\(m_i\)的内积然后softmax得到其匹配度：$$p_i =<br> softmax(u^Tm_i)$$<br><img src="https://yqfile.alicdn.com/ff326d9766310f657d396a1b842c1f8ca9b28936.png" alt="max11"><br>用第三个词嵌入矩阵编码\(x_i\)为\(c_i\)：$$c_i = emedding_C(x<em>i)$$ 计算输出：$$o = \sum\limits</em>{i}p_ic_i$$<br><img src="https://yqfile.alicdn.com/b94928af7827b3d100e6831c38fa76f9e237768f.png" alt="max12"><br>用矩阵\(W(V\times d)\)乘以\(o\)和\(u\)的和。结果传给softmax函数预测最终答案。$$\hat a = softmax(W(o + u))$$<br><img src="https://yqfile.alicdn.com/90cea9061711824f7071e1661d5eecb5364872c6.png" alt="max13"><br>这里，将所有步骤总结为一个图：<br><img src="https://yqfile.alicdn.com/223d41b3a733d068647739db3ca407e66988d75e.png" alt="max1"></p>
<h2 id="多层（Multiple-layer）"><a href="#多层（Multiple-layer）" class="headerlink" title="多层（Multiple layer）"></a>多层（Multiple layer）</h2><p>与RNN类似，可以堆叠多层形成复杂网络。在每一层\(i\)，有它自己的嵌入矩阵\(A_i\)和\(C_i\)。层\(k + 1\)的输入为：$$u^{k + 1} = u^k + o^k$$<br><img src="https://yqfile.alicdn.com/e9313a24aaed40caa62428a5cd2377f3238ebd43.png" alt="max2"></p>
<h2 id="语言模型（Language-model）"><a href="#语言模型（Language-model）" class="headerlink" title="语言模型（Language model）"></a>语言模型（Language model）</h2><p>可以用MemN2N作为语言模型。比如，解析“独立申明”：“We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.”，不是每一句为一个记忆项而是没一词为一项：</p>
<table>
<thead>
<tr>
<th>Memory slot \(m_i\)</th>
<th>word</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>We</td>
</tr>
<tr>
<td>2</td>
<td>hold</td>
</tr>
<tr>
<td>3</td>
<td>these</td>
</tr>
<tr>
<td>4</td>
<td>truths</td>
</tr>
<tr>
<td>5</td>
<td>to</td>
</tr>
<tr>
<td>6</td>
<td>be</td>
</tr>
<tr>
<td>7</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>上述语言模型的目的是预测第7个词。</p>
<p>根据<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="external">MemN2N论文</a>中的描述，其不同有：</p>
<ol>
<li>没有问句，我们试着找下个词而不是问句的答案。因此，我们无需词嵌入矩阵B，只需以常量0.1填充\(u\)。</li>
<li>我们使用多层，但是每层词嵌入矩阵\(A\)相同，而词嵌入矩阵\(B\)不同。</li>
<li>词嵌入增加时间项来记录记忆中词的次序（Section 4.1）。</li>
<li>加上\(o\)前，使\(u\)乘以一个线性向量。</li>
<li>为了帮助训练，对每层的一半单元运用ReLU（Section 5）。</li>
</ol>
<p>以下是构建嵌入\(A\)，\(C\)，\(m_i\)，\(c_i\)，\(p\)，\(o\)和\(\hat a\)的代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_memory</span><span class="params">(self)</span>:</span></div><div class="line">    self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std)) <span class="comment"># Embedding A for sentences</span></div><div class="line">    self.C = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std)) <span class="comment"># Embedding C for sentences</span></div><div class="line">    self.H = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))   <span class="comment"># Multiple it with u before adding to o</span></div><div class="line"></div><div class="line">    <span class="comment"># Sec 4.1: Temporal Encoding to capture the time order of the sentences.</span></div><div class="line">    self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))</div><div class="line">    self.T_C = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))</div><div class="line"></div><div class="line">    <span class="comment"># Sec 2: We are using layer-wise (RNN-like) which the embeddings for each layers are sharing the parameters.</span></div><div class="line">    <span class="comment"># (N, 100, 150) m_i = sum A_ij * x_ij + T_A_i</span></div><div class="line">    m_a = tf.nn.embedding_lookup(self.A, self.sentences)</div><div class="line">    m_t = tf.nn.embedding_lookup(self.T_A, self.T)</div><div class="line">    m = tf.add(m_a, m_t)</div><div class="line"></div><div class="line">    <span class="comment"># (N, 100, 150) c_i = sum C_ij * x_ij + T_C_i</span></div><div class="line">    c_a = tf.nn.embedding_lookup(self.C, self.sentences)</div><div class="line">    c_t = tf.nn.embedding_lookup(self.T_C, self.T)</div><div class="line">    c = tf.add(c_a, c_t)</div><div class="line"></div><div class="line">	<span class="comment"># For each layer</span></div><div class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(self.nhop):</div><div class="line">        u = tf.reshape(self.u_s[<span class="number">-1</span>], [<span class="number">-1</span>, <span class="number">1</span>, self.edim])</div><div class="line">        scores = tf.matmul(u, m, adjoint_b=<span class="keyword">True</span>)</div><div class="line">        scores = tf.reshape(scores, [<span class="number">-1</span>, self.mem_size])</div><div class="line"></div><div class="line">        P = tf.nn.softmax(scores)     <span class="comment"># (N, 100)</span></div><div class="line">        P = tf.reshape(P, [<span class="number">-1</span>, <span class="number">1</span>, self.mem_size])</div><div class="line"></div><div class="line">        o = tf.matmul(P, c)</div><div class="line">        o = tf.reshape(o, [<span class="number">-1</span>, self.edim])</div><div class="line"></div><div class="line">        <span class="comment"># Section 2: We are using layer-wise (RNN-like), so we multiple u with H.</span></div><div class="line">        uh = tf.matmul(self.u_s[<span class="number">-1</span>], self.H)</div><div class="line">        next_u = tf.add(uh, o)</div><div class="line"></div><div class="line">        <span class="comment"># Section 5:  To aid training, we apply ReLU operations to half of the units in each layer.</span></div><div class="line">        F = tf.slice(next_u, [<span class="number">0</span>, <span class="number">0</span>], [self.batch_size, self.lindim])</div><div class="line">	    G = tf.slice(next_u, [<span class="number">0</span>, self.lindim], [self.batch_size, self.edim-self.lindim])</div><div class="line">        K = tf.nn.relu(G)</div><div class="line">        self.u_s.append(tf.concat(axis=<span class="number">1</span>, values=[F, K]))</div><div class="line"></div><div class="line">    self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))</div><div class="line">    z = tf.matmul(self.u_s[<span class="number">-1</span>], self.W)</div></pre></td></tr></table></figure></p>
<p>计算损失，通过梯度修剪构建优化器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)</div><div class="line"></div><div class="line">self.lr = tf.Variable(self.current_lr)</div><div class="line">self.opt = tf.train.GradientDescentOptimizer(self.lr)</div><div class="line"></div><div class="line">params = [self.A, self.C, self.H, self.T_A, self.T_C, self.W]</div><div class="line">grads_and_vars = self.opt.compute_gradients(self.loss, params)</div><div class="line">clipped_grads_and_vars = [(tf.clip_by_norm(gv[<span class="number">0</span>], self.max_grad_norm), gv[<span class="number">1</span>]) \</div><div class="line">                                   <span class="keyword">for</span> gv <span class="keyword">in</span> grads_and_vars]</div><div class="line"></div><div class="line">inc = self.global_step.assign_add(<span class="number">1</span>)</div><div class="line"><span class="keyword">with</span> tf.control_dependencies([inc]):</div><div class="line">      self.optim = self.opt.apply_gradients(clipped_grads_and_vars)</div></pre></td></tr></table></figure></p>
<p>训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data)</span>:</span></div><div class="line">    n_batch = int(math.ceil(len(data) / self.batch_size))</div><div class="line">    cost = <span class="number">0</span></div><div class="line"></div><div class="line">    u = np.ndarray([self.batch_size, self.edim], dtype=np.float32)      <span class="comment"># (N, 150) Will fill with 0.1</span></div><div class="line">    T = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)    <span class="comment"># (N, 100) Will fill with 0..99</span></div><div class="line">    target = np.zeros([self.batch_size, self.nwords])                   <span class="comment"># one-hot-encoded</span></div><div class="line">    sentences = np.ndarray([self.batch_size, self.mem_size])</div><div class="line"></div><div class="line">    u.fill(self.init_u)   <span class="comment"># (N, 150) Fill with 0.1 since we do not need query in the language model.</span></div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(self.mem_size):   <span class="comment"># (N, 100) 100 memory cell with 0 to 99 time sequence.</span></div><div class="line">       T[:,t].fill(t)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(n_batch):</div><div class="line">        target.fill(<span class="number">0</span>)      <span class="comment"># (128, 10,000)</span></div><div class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> range(self.batch_size):</div><div class="line">            <span class="comment"># We random pick a word in our data and use that as the word we need to predict using the language model.</span></div><div class="line">            m = random.randrange(self.mem_size, len(data))</div><div class="line">            target[b][data[m]] = <span class="number">1</span>                       <span class="comment"># Set the one hot vector for the target word to 1</span></div><div class="line"></div><div class="line">            <span class="comment"># (N, 100). Say we pick word 1000, we then fill the memory using words 1000-150 ... 999</span></div><div class="line">            <span class="comment"># We fill Xi (sentence) with 1 single word according to the word order in data.</span></div><div class="line">           sentences[b] = data[m - self.mem_size:m]</div><div class="line"></div><div class="line">            _, loss, self.step = self.sess.run([self.optim,</div><div class="line">                                                self.loss,</div><div class="line">                                                self.global_step],</div><div class="line">                                                feed_dict=&#123;</div><div class="line">                                                    self.u: u,</div><div class="line">                                                    self.T: T,</div><div class="line">                                                    self.target: target,</div><div class="line">                                                    self.sentences: sentences&#125;)</div><div class="line">            cost += np.sum(loss)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost/n_batch/self.batch_size</div></pre></td></tr></table></figure></p>
<p>初始化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MemN2N</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config, sess)</span>:</span></div><div class="line">        self.nwords = config.nwords         <span class="comment"># 10,000</span></div><div class="line">        self.init_u = config.init_u         <span class="comment"># 0.1 (We don't need a query in language model. So set u to be 0.1</span></div><div class="line">        self.init_std = config.init_std     <span class="comment"># 0.05</span></div><div class="line">        self.batch_size = config.batch_size <span class="comment"># 128</span></div><div class="line">        self.nepoch = config.nepoch         <span class="comment"># 100</span></div><div class="line">        self.nhop = config.nhop             <span class="comment"># 6</span></div><div class="line">        self.edim = config.edim             <span class="comment"># 150</span></div><div class="line">        self.mem_size = config.mem_size     <span class="comment"># 100</span></div><div class="line">        self.lindim = config.lindim         <span class="comment"># 75</span></div><div class="line">        self.max_grad_norm = config.max_grad_norm   <span class="comment"># 50</span></div><div class="line"></div><div class="line">        self.show = config.show</div><div class="line">        self.is_test = config.is_test</div><div class="line">        self.checkpoint_dir = config.checkpoint_dir</div><div class="line"></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(self.checkpoint_dir):</div><div class="line">            <span class="keyword">raise</span> Exception(<span class="string">" [!] Directory %s not found"</span> % self.checkpoint_dir)</div><div class="line"></div><div class="line">        <span class="comment"># (?, 150) Unlike Q&amp;A, the language model do not need a query (or care what is its value).</span></div><div class="line">        <span class="comment"># So we bypass q and fill u directly with 0.1 later.</span></div><div class="line">        self.u = tf.placeholder(tf.float32, [<span class="keyword">None</span>, self.edim], name=<span class="string">"u"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># (?, 100) Sec. 4.1, we add temporal encoding to capture the time sequence of the memory Xi.</span></div><div class="line">        self.T = tf.placeholder(tf.int32, [<span class="keyword">None</span>, self.mem_size], name=<span class="string">"T"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># (N, 10000) The answer word we want. (Next word for the language model)</span></div><div class="line">        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=<span class="string">"target"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># (N, 100) The memory Xi. For each sentence here, it contains 1 single word only.</span></div><div class="line">        self.sentences = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=<span class="string">"sentences"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Store the value of u at each layer</span></div><div class="line">        self.u_s = []</div><div class="line">        self.u_s.append(self.u)</div></pre></td></tr></table></figure></p>
<p>完整代码在<a href="https://github.com/jhui/machine_learning/tree/master/MemN2N" target="_blank" rel="external">github</a></p>
<h1 id="动态记忆网络（Dynamic-memory-network）"><a href="#动态记忆网络（Dynamic-memory-network）" class="headerlink" title="动态记忆网络（Dynamic memory network）"></a>动态记忆网络（Dynamic memory network）</h1><p><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="external">Source</a><br><img src="https://yqfile.alicdn.com/276c84b594b60f8a12bbfb98b4e001a2e905e986.png" alt="dmm"></p>
<h2 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h2><p>句子由GRU处理，其中最后隐藏状态用于记忆模块。</p>
<h2 id="Question-module"><a href="#Question-module" class="headerlink" title="Question module"></a>Question module</h2><p>$$q_t = GRU(v<em>t, q</em>{t-1})$$</p>
<h2 id="Episodic-memory-module"><a href="#Episodic-memory-module" class="headerlink" title="Episodic memory module"></a>Episodic memory module</h2><p>$$h_i^t = g_i^tGRU(s<em>i, h</em>{i-1}^t) + (1 - g<em>i^t)h</em>{i-1}^t$$ 其中\(s_i\)表示第\(i\)个句子，\(h_i^t\)表示处理句子\(i\)第\(t\)步的隐藏状态，其最后隐藏状态为\(m^\prime\)</p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep learning</a></li></ul>

    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

  


</section>
        
      </div>
      
        <div align="center" style="margin-top: 30px;"><hr class="hr" style="margin:0px; height:3px;"></div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2013 - 2017 莫名的blog All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>


  <script src="/js/home.js"></script>










	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            莫名的blog
          </div>
          <div class="panel-body">
            Copyright © 2017 莫名 All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>